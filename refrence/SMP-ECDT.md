# SMP-ECDT



## SMP-ECDT 2017

[SMP-ECDT 2017](http://ir.hit.edu.cn/smp2017-ecdt)，[评测数据](https://github.com/HITlilingzhi/SMP2017ECDT-DATA)



### 任务1：用户意图领域分类

在人机对话系统地应用过程中，用户可能会有多种意图，相应地会触发人机对话系统中的多个领域，其中包括任务型垂直领域（如查询机票、酒店、公交车等）、知识型问答以及闲聊等。因而，人机对话系统地一个关键任务就是正确地将用户的输入分类到相应地领域中，从而才能返回正确的回复结果。

评测任务中包含闲聊和垂类两大类，其中垂类又细分为30个垂直领域。本次评测任务中，仅考虑针对单对话轮用户意图的领域分类，多轮对话整体意图的领域分类不在此评测范围之内。

评测方式分为封闭式评测（仅允许使用主办方提供的评测数据进行训练和开发）和开放式评测（可以开放地获取除主办方提供的数据之外的训练及开发数据）



### 任务2：特定域任务型人机对话在线评测

针对特定域的任务型人机对话系统，人工实时在线对话交互及评价能够反映系统对于用户意图响应的性能以及用户真实的满意度。

本次评测任务2的特定领域包括：机票类、火车票类和酒店类3个垂直领域，系统通过与测试人员实时在线对话完成相应的预定或查询任务，满足测试人员的需求。



## SMP-ECDT 2018

[SMP-ECDT 2018](https://www.steamedfish.cn/2018/09/30/%E3%80%90%E7%B2%BE%E5%BD%A9%E7%BB%A7%E7%BB%AD%E3%80%91SMP2018%E4%B8%AD%E6%96%87%E4%BA%BA%E6%9C%BA%E5%AF%B9%E8%AF%9D%E6%8A%80%E6%9C%AF%E8%AF%84%E6%B5%8B%EF%BC%88ECDT%EF%BC%89%E5%88%B7%E6%A6%9C%E5%85%AC%E5%91%8A/)

[SMP2018中文人机对话技术评测 论文摘要](https://www.jiqizhixin.com/articles/2019-03-06-13)



### 任务1：用户意图领域分类

包含闲聊和垂类两大类，其中垂类又细分为30个垂直领域，参赛系统需要判定用户的输入所属的类别。



### 任务2：特定域任务型人机对话在线评测

参赛系统通过与测试人员实时在线对话完成相应的预定或查询任务，完成测试人员的特定域需求。

具体描述为：对于一个查询并预定机票、火车票或酒店的一个或多个意图的混合任务，根据给定的数据库数据构建系统引导用户完成相应的任务。本次评测仍采用人工在线评测的方法，一个完整意图描述为：“预定明天从北京去上海的上午或者中午出发的机票”，其订机票过程如表2所示：

| Agent | Response                                                     |
| ----- | ------------------------------------------------------------ |
| User  | 查询明天从北京去上海的机票                                   |
| Robot | 请问您需要订票吗？                                           |
| User  | 是的                                                         |
| Robot | 请问您要明天什么时候出发呢？                                 |
| User  | 上午或中午吧                                                 |
| Robot | 帮您查询了明天从北京到上海的机票，没有上午出发的航班，中午12:10出发是否可以？7.3折，910元 |
| User  | 也行，就订这个吧                                             |
| Robot | 已经帮您预订该航班机票，将跳转至付款页面                     |

为了保证任务型对话系统各个方面的主观和客观性能，本次评测采用了以下指标：

- 任务完成率：每个测试对话看作是一个任务，测试过程中完成的任务数占任务总数的比率。
- 平均对话轮数：完成一个任务所产生的对话句子数，在完成任务的前提下越少越好。
- 用户满意度：评测员对系统的客观打分，包含5个取值{-2, -1, 0, 1, 2}
- 回复流畅度：主观打分，包含3个取值{-1, 0, 1}
- 未覆盖数据引导能力：主观打分，包含2个取值{0, 1}



## SMP-ECDT 2019

[SMP-ECDT 2019](http://conference.cipsc.org.cn/smp2019/evaluation.html)



### 任务1：自然语言理解评测

本次评测包括领域分类、意图识别和语义槽填充三项子任务，例如给定一个用户的表达句子“我想订上海飞往北京的航班”，则该句的：

- 领域为：“机票”
- 意图为：“订机票”
- 语义槽为：
  - departCity：上海
  - arriveCity：北京

本次评测包括单轮对话用户意图的领域分类、意图识别和语义槽填充任务，多轮对话整体意图的理解不在此次评测范围之内。



### 任务2：个性化对话竞赛

略……（皓宇师兄介绍ConvAI2）

在对话场景下，已知对话上下文和所有对话参与者的个性化属性，要求生成符合给定个性化特征与上下文逻辑的回复R。

所谓个性化属性由一系列键值对（如<性别，男>，<年龄，90后>）描述：
$$
S=<k_1, v_1>, <k_2, v_2>, \cdots, <k_n ,v_n>
$$
所生成的回复R需要足够流畅、与对话上下文语义相关并且符合所指定的发话人个性化特征。



## SMP-ECDT 2020

[SMP-ECDT 2020](https://smp2020.aconf.cn/smp.html#3)



### 任务1：小样本对话语言理解技术评测

对话语言理解SLU（Spoken Language Understanding）是任务型对话系统的关键组合模块，它把用户的自然语言输入（Utterance）转化为结构化信息（Semantic Frame）以为后续的对话状态管理和回复生成提供支持。其中 Semantic Frame 包括用户意图（Intent）和语义槽（Slot）。

区别于普通的对话语言理解，本评测关注小样本学习场景，即每个测试类别只有几个标注样例。具体任务如图三所示，模型先在一些数据充足的领域训练，然后在未见的新领域上测试。针对一个领域，我们每次给定模型一个带标注的支撑样本集（Support Set）作为参考，让模型对任意未见过的查询样本集（Query Set）标注用户意图和槽位。以图中测试领域为例，给定Support Set，和Query句“播放阿凡达”，模型需要预测出意图为“播放电影”，槽位为【电影：阿凡达】。（通常训练时为模拟小样本情形，训练领域数据也会构造为Support-Query形式训练模型，这里不做强制要求）

![](https://file.aconf.org/conf/hz/2020/03/174917/images/007S8ZIlly1gehkxmifxkj319g0gsk3n.jpg)



### 任务2：知识驱动的多轮对话竞赛

在对话场景下，已知对话上下文和所有知识图谱信息，要求生成符合知识图谱信息与上下文逻辑的对话回复。

知识图谱由一系列三元组（如<头实体，关系，尾实体>）描述。

所生成的对话回复需要足够流畅、与对话上下文语义相关并且符合相关的知识图谱信息。